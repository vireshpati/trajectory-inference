{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for experiments with colored MNIST and Celeba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.enot import SDE, integrate, make_net\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import gc\n",
    "import pdb\n",
    "\n",
    "from src import distributions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.resnet2 import ResNet_D\n",
    "from src.cunet import CUNet\n",
    "from src.improved_diffusion import UNetModel\n",
    "\n",
    "from src.tools import unfreeze, freeze\n",
    "from src.tools import load_dataset, get_sde_pushed_loader_stats\n",
    "from src.fid_score import calculate_frechet_distance\n",
    "from src.tools import weights_init_D\n",
    "from src.plotters import plot_random_sde_images, plot_fixed_sde_images, plot_fixed_sde_trajectories, plot_random_sde_trajectories, plot_several_fixed_sde_trajectories, plot_several_random_sde_trajectories\n",
    "\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import wandb\n",
    "from src.tools import fig2data, fig2img # for wandb\n",
    "\n",
    "# This needed to use dataloaders for some datasets\n",
    "from PIL import PngImagePlugin\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import wandb\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config\n",
    "\n",
    "Dataset choosing in the first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# For Celeba exps\n",
    "DATASET1, DATASET1_PATH = 'CelebA_low', '/gpfs/data/gpfs0/n.gushchin/img_align_celeba'\n",
    "DATASET2, DATASET2_PATH = 'CelebA_high', '/gpfs/data/gpfs0/n.gushchin/img_align_celeba'\n",
    "\n",
    "# For Colored MNIST exps\n",
    "# DATASET1, DATASET1_PATH = 'MNIST-colored_2', '/gpfs/data/gpfs0/n.gushchin/'\n",
    "# DATASET2, DATASET2_PATH = 'MNIST-colored_3', '/gpfs/data/gpfs0/n.gushchin/'\n",
    "\n",
    "# We use epsilon in [0, 1, 10]\n",
    "EPSILON = 1\n",
    "\n",
    "# N steps in the Euler-Maruyama\n",
    "N_STEPS = 10\n",
    "\n",
    "# GPU choosing\n",
    "DEVICE_IDS = [0]\n",
    "GPU_DEVICE = 0\n",
    "\n",
    "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
    "\n",
    "T_ITERS = 10\n",
    "D_LR, T_LR = 1e-4, 1e-4\n",
    "BETA_D, BETA_T = 0.9, 0.9\n",
    "IMG_SIZE = 64\n",
    "UNET_BASE_FACTOR = 128\n",
    "\n",
    "TIME_DIM = 128\n",
    "CONSTANT_TIME = False\n",
    "USE_POSITIONAL_ENCODING = True\n",
    "RESNET_GENERATOR = False\n",
    "INTEGRAL_SCALE = 1/(3*IMG_SIZE*IMG_SIZE)\n",
    "ONE_STEP_INIT_ITERS = 0\n",
    "T_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "D_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "PREDICT_SHIFT = True\n",
    "SMART_INTERVALS = False\n",
    "INTERVAL_SHRINK_START_TIME = 0.98\n",
    "LAST_STEP_NOISE_STD = 1e-3\n",
    "USE_GRADIENT_CHECKPOINT = False\n",
    "PREDICT_NOISE_AT_LAST_STEP = False\n",
    "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
    "TRACK_VAR_INTERVAL = 10\n",
    "IMPROVED_DIFFUSION = False\n",
    "USE_CHECKPOINTS_INSIDE_MODEL = False\n",
    "EPSILON_SCHEDULER_LAST_ITER = 20000\n",
    "USE_EXPONENTIAL_AVERAGE_MODEL = False\n",
    "DISTINCT_SHIFT_MODELS = False\n",
    "IMAGE_INPUT = True\n",
    "\n",
    "DATASET1_CHANNELS = 3\n",
    "DATASET2_CHANNELS = 3\n",
    "GRAY_PLOTS = False\n",
    "BATCH_SIZE = 64\n",
    "STEPS_TO_SHOW = 10\n",
    "\n",
    "PLOT_INTERVAL = 500\n",
    "COST = 'schrodinger'\n",
    "CPKT_INTERVAL = 500\n",
    "MAX_STEPS = 100001\n",
    "SEED = 0xBADBEEF\n",
    "\n",
    "GAMMA0, GAMMA1 = 0.0, 0.333\n",
    "GAMMA_ITERS = 20000\n",
    "\n",
    "CONTINUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = f'{DATASET1}_{DATASET2}_INTEGRAL_SCALE_{round(INTEGRAL_SCALE, 5)}_T_ITERS_{T_ITERS}_BATCH_SIZE_{BATCH_SIZE}_DISTINCT_SHIFT_MODELS_{DISTINCT_SHIFT_MODELS}_PREDICT_SHIFT_{PREDICT_SHIFT}_BASE_FACTOR_{UNET_BASE_FACTOR}_STEPS_{N_STEPS}_EPSILON_{EPSILON}_T_LR_{T_LR}_D_LR_{D_LR}_BETA_T_{BETA_T}_BETA_D_{BETA_D}_IMG_SIZE{IMG_SIZE}'\n",
    "OUTPUT_PATH = '../checkpoints/{}/{}/{}_{}_{}/'.format(COST, EXP_NAME, DATASET1, DATASET2, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    SEED=SEED,\n",
    "    DATASET1=DATASET1,\n",
    "    DATASET2=DATASET2, \n",
    "    T_ITERS=T_ITERS,\n",
    "    D_LR=D_LR, T_LR=T_LR,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    UNET_BASE_FACTOR=UNET_BASE_FACTOR,\n",
    "    N_STEPS=N_STEPS,\n",
    "    EPSILON=EPSILON,\n",
    "    CONSTANT_TIME=CONSTANT_TIME,\n",
    "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
    "    TIME_DIM=TIME_DIM,\n",
    "    RESNET_GENERATOR=RESNET_GENERATOR,\n",
    "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
    "    ONE_STEP_INIT_ITERS=ONE_STEP_INIT_ITERS,\n",
    "    T_GRADIENT_MAX_NORM=T_GRADIENT_MAX_NORM,\n",
    "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
    "    PREDICT_SHIFT=PREDICT_SHIFT,\n",
    "    SMART_INTERVALS=SMART_INTERVALS,\n",
    "    INTERVAL_SHRINK_START_TIME=INTERVAL_SHRINK_START_TIME,\n",
    "    LAST_STEP_NOISE_STD=LAST_STEP_NOISE_STD,\n",
    "    USE_GRADIENT_CHECKPOINT=USE_GRADIENT_CHECKPOINT,\n",
    "    PREDICT_NOISE_AT_LAST_STEP=PREDICT_NOISE_AT_LAST_STEP,\n",
    "    N_LAST_STEPS_WITHOUT_NOISE=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "    TRACK_VAR_INTERVAL=TRACK_VAR_INTERVAL,\n",
    "    IMPROVED_DIFFUSION=IMPROVED_DIFFUSION,\n",
    "    USE_CHECKPOINTS_INSIDE_MODEL=USE_CHECKPOINTS_INSIDE_MODEL,\n",
    "    EPSILON_SCHEDULER_LAST_ITER=EPSILON_SCHEDULER_LAST_ITER,\n",
    "    USE_EXPONENTIAL_AVERAGE_MODEL=USE_EXPONENTIAL_AVERAGE_MODEL,\n",
    "    DISTINCT_SHIFT_MODELS=DISTINCT_SHIFT_MODELS,\n",
    ")\n",
    "\n",
    "AUGMENTED_DATASETS = ['dtd']\n",
    "FID_EPOCHS = 50 if DATASET1 in AUGMENTED_DATASETS else 1\n",
    "    \n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(f'cuda:{DEVICE_IDS[0]}')\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../stats/{}_{}_test.json'.format(DATASET2, IMG_SIZE)\n",
    "with open(filename, 'r') as fp:\n",
    "    data_stats = json.load(fp)\n",
    "    mu_data, sigma_data = data_stats['mu'], data_stats['sigma']\n",
    "del data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampler, X_test_sampler = load_dataset(DATASET1, DATASET1_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=8)\n",
    "Y_sampler, Y_test_sampler = load_dataset(DATASET2, DATASET2_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    \n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
    "D.apply(weights_init_D)\n",
    "\n",
    "T = CUNet(DATASET1_CHANNELS, DATASET2_CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR).cuda()\n",
    "\n",
    "T = SDE(shift_model=T, epsilon=EPSILON, n_steps=N_STEPS,\n",
    "        time_dim=TIME_DIM, n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
    "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
    "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
    "        predict_shift=PREDICT_SHIFT, image_input=IMAGE_INPUT).cuda()\n",
    "\n",
    "if len(DEVICE_IDS) > 1 and CONTINUE==-1:\n",
    "    T = nn.DataParallel(T, device_ids=DEVICE_IDS)\n",
    "    D = nn.DataParallel(D, device_ids=DEVICE_IDS)\n",
    "    \n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('D params:', np.sum([np.prod(p.shape) for p in D.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_opt = torch.optim.Adam(T.parameters(), lr=T_LR, weight_decay=1e-10, betas=(BETA_T, 0.999))\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=D_LR, weight_decay=1e-10, betas=(BETA_D, 0.999))\n",
    "T_scheduler = torch.optim.lr_scheduler.MultiStepLR(T_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5)\n",
    "D_scheduler = torch.optim.lr_scheduler.MultiStepLR(D_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5)\n",
    "\n",
    "if CONTINUE > -1:\n",
    "    T_opt.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'T_opt_{SEED}_{CONTINUE}.pt')))\n",
    "    T_scheduler.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'T_scheduler_{SEED}_{CONTINUE}.pt')))\n",
    "    \n",
    "    T.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'T_{SEED}_{CONTINUE}.pt')))\n",
    "    D.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'D_{SEED}_{CONTINUE}.pt')))\n",
    "    \n",
    "    if len(DEVICE_IDS) > 1:\n",
    "        T = nn.DataParallel(T, device_ids=DEVICE_IDS)\n",
    "        D = nn.DataParallel(D, device_ids=DEVICE_IDS)\n",
    "    \n",
    "    D_opt.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'D_opt_{SEED}_{CONTINUE}.pt')))\n",
    "    D_scheduler.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'D_scheduler_{SEED}_{CONTINUE}.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0xBADBEEF); np.random.seed(0xBADBEEF)\n",
    "X_fixed, Y_fixed = X_sampler.sample(10), Y_sampler.sample(10)\n",
    "\n",
    "X_test_fixed, Y_test_fixed = X_test_sampler.sample(10), Y_test_sampler.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=EXP_NAME, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training cycle and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epsilon_scheduler = lambda step: min(EPSILON, EPSILON*(step/(EPSILON_SCHEDULER_LAST_ITER)))\n",
    "\n",
    "for step in tqdm(range(CONTINUE + 1, MAX_STEPS)):\n",
    "    \n",
    "    unfreeze(T); freeze(D)\n",
    "    T.train()\n",
    "    \n",
    "    new_epsilon = epsilon_scheduler(step)\n",
    "    if len(DEVICE_IDS) > 1:\n",
    "        T.module.set_epsilon(new_epsilon)\n",
    "    else:\n",
    "        T.set_epsilon(new_epsilon)\n",
    "    wandb.log({f'Epsilon' : new_epsilon}, step=step)\n",
    "    \n",
    "    for t_iter in range(T_ITERS):\n",
    "        T_opt.zero_grad()\n",
    "        \n",
    "        X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "        X0.requires_grad_()\n",
    "        \n",
    "        trajectory, times, shifts = T(X0)\n",
    "        XN = trajectory[:, -1]\n",
    "        norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1)**2\n",
    "        integral = INTEGRAL_SCALE*integrate(norm, times[0])\n",
    "        \n",
    "        T_loss = (integral + D(X1) - D(XN)).mean()\n",
    "        T_loss.backward()\n",
    "        T_gradient_norm = torch.nn.utils.clip_grad_norm_(T.parameters(), max_norm=T_GRADIENT_MAX_NORM)\n",
    "        T_opt.step()\n",
    "        \n",
    "        if USE_EXPONENTIAL_AVERAGE_MODEL:\n",
    "            ema_T.shift_model.update()\n",
    "    \n",
    "    \n",
    "    wandb.log({f'T gradient norm' : T_gradient_norm.item()}, step=step)\n",
    "    wandb.log({f'Mean norm' : torch.sqrt(norm).mean().item()}, step=step)\n",
    "    wandb.log({f'T_loss' : T_loss.item()}, step=step)\n",
    "        \n",
    "    T_scheduler.step()\n",
    "    del T_loss, X0, X1, XN; gc.collect(); torch.cuda.empty_cache()\n",
    "        \n",
    "    freeze(T); unfreeze(D)\n",
    "    \n",
    "    D_opt.zero_grad()\n",
    "    \n",
    "    X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "    trajectory, times, shifts = T(X0)\n",
    "    XN = trajectory[:, -1]\n",
    "    norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1)**2\n",
    "    integral = INTEGRAL_SCALE*integrate(norm, times[0])\n",
    "    \n",
    "    D_X1 = D(X1)\n",
    "    D_XN = D(XN)\n",
    "    \n",
    "    D_loss = (-integral - D_X1 + D_XN).mean()\n",
    "    D_loss.backward()\n",
    "    D_gradient_norm = torch.nn.utils.clip_grad_norm_(D.parameters(), max_norm=D_GRADIENT_MAX_NORM)\n",
    "    D_opt.step()\n",
    "    D_scheduler.step()\n",
    "    \n",
    "    wandb.log({f'D gradient norm' : D_gradient_norm.item()}, step=step)\n",
    "    wandb.log({f'D_loss' : D_loss.item()}, step=step)\n",
    "    \n",
    "    wandb.log({f'integral' : integral.mean().item()}, step=step)\n",
    "    wandb.log({f'D_X1' : D_X1.mean().item()}, step=step)\n",
    "    wandb.log({f'D_XN' : D_XN.mean().item()}, step=step)\n",
    "    del D_loss, X0, X1, XN; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    if step % TRACK_VAR_INTERVAL == 0:\n",
    "        trajectories = []\n",
    "        X0 = X_sampler.sample(BATCH_SIZE)\n",
    "        X1 = Y_sampler.sample(BATCH_SIZE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(4):\n",
    "                trajectory, times, shifts = T(X0)\n",
    "                trajectories.append(trajectory.detach())\n",
    "                \n",
    "        trajectories = torch.stack(trajectories, dim=0)\n",
    "        \n",
    "        for i in range(1, trajectories.shape[2]):\n",
    "            wandb.log({f'T_{i} var' : trajectories[:, :, i].var(dim=0).mean().item()}, step=step)\n",
    "            \n",
    "            \n",
    "        for i in range(0, trajectories.shape[2]):\n",
    "            wandb.log({f'T_{i} unconditional var' : trajectories[0, :, i].var(dim=0).mean().item()}, step=step)\n",
    "            \n",
    "        for i in range(0, trajectories.shape[2]):\n",
    "            wandb.log({f'T_{i} var of conditional mean' : trajectories[:, :, i].mean(dim=0).var(dim=0).mean().item()}, step=step)\n",
    "        \n",
    "        T_var = trajectories[:, :, -1].var(dim=0).mean()\n",
    "        wandb.log({f'T var' : T_var.item()}, step=step)\n",
    "        \n",
    "        T_unconditional_var = trajectories[0, :, -1].var(dim=0).mean()\n",
    "        wandb.log({f'T unconditional var' : T_unconditional_var.item()}, step=step)\n",
    "        \n",
    "        X1_var = X1.var(dim=0).mean()\n",
    "        wandb.log({f'X_1 var' : X1_var.item()}, step=step)\n",
    "        \n",
    "        if DATASET1_CHANNELS == 3:\n",
    "            for i in range(0, trajectories.shape[2]):\n",
    "                wandb.log({f'T_{i} red mean' : trajectories[0, :, i, 0].mean().item()}, step=step)\n",
    "                wandb.log({f'T_{i} green mean' : trajectories[0, :, i, 1].mean().item()}, step=step)\n",
    "                wandb.log({f'T_{i} blue mean' : trajectories[0, :, i, 2].mean().item()}, step=step)\n",
    "\n",
    "            wandb.log({f'X_1 red mean' : X1[:, 0].mean().item()}, step=step)\n",
    "            wandb.log({f'X_1 green mean' : X1[:, 1].mean().item()}, step=step)\n",
    "            wandb.log({f'X_1 blue mean' : X1[:, 2].mean().item()}, step=step)\n",
    "        \n",
    "            \n",
    "    if step % PLOT_INTERVAL == 0:\n",
    "        print('Plotting')\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        inference_T = T\n",
    "        if USE_EXPONENTIAL_AVERAGE_MODEL:\n",
    "            inference_T = ema_T\n",
    "            \n",
    "        inference_T.eval()\n",
    "        \n",
    "        fig, axes = plot_fixed_sde_images(X_fixed, Y_fixed, inference_T, gray=GRAY_PLOTS)\n",
    "        wandb.log({'Fixed Images' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "        plt.show(fig); plt.close(fig)\n",
    "        \n",
    "        fig, axes = plot_random_sde_images(X_sampler, Y_sampler, inference_T, gray=GRAY_PLOTS)\n",
    "        wandb.log({'Random Images' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "        plt.show(fig); plt.close(fig)\n",
    "        \n",
    "        fig, axes = plot_fixed_sde_images(X_test_fixed, Y_test_fixed, inference_T, gray=GRAY_PLOTS)\n",
    "        wandb.log({'Fixed Test Images' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "        plt.show(fig); plt.close(fig) \n",
    "        \n",
    "        fig, axes = plot_random_sde_images(X_test_sampler,  Y_test_sampler, inference_T, gray=GRAY_PLOTS)\n",
    "        wandb.log({'Random Test Images' : [wandb.Image(fig2img(fig))]}, step=step)\n",
    "        plt.show(fig); plt.close(fig)\n",
    "        \n",
    "        if step >= ONE_STEP_INIT_ITERS:\n",
    "            steps_to_draw = min(N_STEPS, 10)\n",
    "            fig, axes = plot_fixed_sde_trajectories(X_fixed, Y_fixed, inference_T, STEPS_TO_SHOW, N_STEPS, gray=GRAY_PLOTS)\n",
    "            wandb.log({'Fixed Trajectories' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "            plt.show(fig); plt.close(fig)\n",
    "\n",
    "            fig, axes = plot_random_sde_trajectories(X_sampler, Y_sampler, inference_T, STEPS_TO_SHOW, N_STEPS, gray=GRAY_PLOTS)\n",
    "            wandb.log({'Random Trajectories' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "            plt.show(fig); plt.close(fig)\n",
    "\n",
    "            fig, axes = plot_fixed_sde_trajectories(X_test_fixed, Y_test_fixed, inference_T, STEPS_TO_SHOW, N_STEPS, gray=GRAY_PLOTS)\n",
    "            wandb.log({'Fixed Test Trajectories' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "            plt.show(fig); plt.close(fig) \n",
    "\n",
    "            fig, axes = plot_random_sde_trajectories(X_test_sampler,  Y_test_sampler, inference_T, STEPS_TO_SHOW, N_STEPS, gray=GRAY_PLOTS)\n",
    "            wandb.log({'Random Test Trajectories' : [wandb.Image(fig2img(fig))]}, step=step)\n",
    "            plt.show(fig); plt.close(fig)\n",
    "            \n",
    "            fig, axes = plot_several_fixed_sde_trajectories(X_test_fixed, Y_test_fixed, inference_T, STEPS_TO_SHOW, N_STEPS, gray=GRAY_PLOTS)\n",
    "            wandb.log({'Several Fixed Trajectories' : [wandb.Image(fig2img(fig))]}, step=step) \n",
    "            plt.show(fig); plt.close(fig) \n",
    "\n",
    "            fig, axes = plot_several_random_sde_trajectories(X_test_sampler,  Y_test_sampler, inference_T, STEPS_TO_SHOW, N_STEPS, gray=GRAY_PLOTS)\n",
    "            wandb.log({'Several Random Trajectories' : [wandb.Image(fig2img(fig))]}, step=step)\n",
    "            plt.show(fig); plt.close(fig)\n",
    "    \n",
    "    if step % CPKT_INTERVAL == 0:\n",
    "        inference_T = T\n",
    "        if USE_EXPONENTIAL_AVERAGE_MODEL:\n",
    "            inference_T = ema_T\n",
    "        \n",
    "        inference_T.eval()\n",
    "        freeze(T);\n",
    "        \n",
    "        if len(DEVICE_IDS) > 1:\n",
    "            torch.save(T.module.state_dict(), os.path.join(OUTPUT_PATH, f'T_{SEED}_{step}.pt'))\n",
    "            torch.save(D.module.state_dict(), os.path.join(OUTPUT_PATH, f'D_{SEED}_{step}.pt'))\n",
    "        else:\n",
    "            torch.save(T.state_dict(), os.path.join(OUTPUT_PATH, f'T_{SEED}_{step}.pt'))\n",
    "            torch.save(D.state_dict(), os.path.join(OUTPUT_PATH, f'D_{SEED}_{step}.pt'))\n",
    "        if USE_EXPONENTIAL_AVERAGE_MODEL:\n",
    "            torch.save(ema_T.state_dict(), os.path.join(OUTPUT_PATH, f'ema_T_{SEED}_{step}.pt'))\n",
    "        torch.save(D_opt.state_dict(), os.path.join(OUTPUT_PATH, f'D_opt_{SEED}_{step}.pt'))\n",
    "        torch.save(T_opt.state_dict(), os.path.join(OUTPUT_PATH, f'T_opt_{SEED}_{step}.pt'))\n",
    "        torch.save(D_scheduler.state_dict(), os.path.join(OUTPUT_PATH, f'D_scheduler_{SEED}_{step}.pt'))\n",
    "        torch.save(T_scheduler.state_dict(), os.path.join(OUTPUT_PATH, f'T_scheduler_{SEED}_{step}.pt'))\n",
    "        \n",
    "        print('Computing FID')\n",
    "        mu, sigma = get_sde_pushed_loader_stats(inference_T, X_test_sampler.loader,\n",
    "                                                n_epochs=FID_EPOCHS, batch_size=BATCH_SIZE)\n",
    "        fid = calculate_frechet_distance(mu_data, sigma_data, mu, sigma)\n",
    "        wandb.log({f'FID (Test)' : fid}, step=step)\n",
    "        del mu, sigma\n",
    "    \n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
