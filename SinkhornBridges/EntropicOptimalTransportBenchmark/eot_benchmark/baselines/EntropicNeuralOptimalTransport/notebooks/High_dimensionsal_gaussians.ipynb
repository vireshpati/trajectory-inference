{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Gaussian optimal Transport in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import math\n",
    "import gc\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "torch.random.manual_seed(0xBADBEEF)\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.linalg import sqrtm, inv\n",
    "\n",
    "# from src.icnn import DenseICNN\n",
    "# from src.tools import compute_l1_norm, ewma\n",
    "from src.fid_score import calculate_frechet_distance\n",
    "from src import distributions\n",
    "from src.tools import unfreeze, freeze\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 2\n",
    "assert DIM > 1\n",
    "\n",
    "OUTPUT_SEED = 0xC0FFEE\n",
    "L1 = 1e-10\n",
    "GPU_DEVICE = 3\n",
    "BATCH_SIZE = 512\n",
    "EPSILON = 10\n",
    "N_STEPS = 50\n",
    "TIME_DIM = 1\n",
    "T_LR = 3e-4\n",
    "D_LR = 3e-4\n",
    "T_ITERS = 10\n",
    "CONSTANT_TIME = False\n",
    "USE_POSITIONAL_ENCODING = False\n",
    "INTEGRAL_SCALE = 1/(DIM)\n",
    "T_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "D_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "IS_RESNET_GENERATOR = False\n",
    "PREDICT_SHIFT = True\n",
    "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
    "\n",
    "T_N_HIDDEN = 512\n",
    "T_N_LAYERS = 3\n",
    "\n",
    "D_N_HIDDEN = 512\n",
    "D_N_LAYERS = 3\n",
    "\n",
    "MAX_STEPS = 10000\n",
    "CONTINUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = f'Gaussians_test_EPSILON_{EPSILON}_STEPS_{N_STEPS}_DIM_{DIM}'\n",
    "\n",
    "config = dict(\n",
    "    DIM=DIM,\n",
    "    T_ITERS=T_ITERS,\n",
    "    D_LR=D_LR, T_LR=T_LR,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    N_STEPS=N_STEPS,\n",
    "    EPSILON=EPSILON,\n",
    "    CONSTANT_TIME=CONSTANT_TIME,\n",
    "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
    "    TIME_DIM=TIME_DIM,\n",
    "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
    "    T_GRADIENT_MAX_NORM=T_GRADIENT_MAX_NORM,\n",
    "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
    "    IS_RESNET_GENERATOR=IS_RESNET_GENERATOR,\n",
    "    T_N_HIDDEN=T_N_HIDDEN,\n",
    "    T_N_LAYERS=T_N_LAYERS,\n",
    "    D_N_HIDDEN=D_N_HIDDEN,\n",
    "    D_N_LAYERS=D_N_LAYERS,\n",
    "    PREDICT_SHIFT=PREDICT_SHIFT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "torch.cuda.set_device(GPU_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Wasserstein-2 Distance:  1.9547454909035862\n",
      "Variance of X: 4.264332\n",
      "Variance of Y: 4.24657\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(OUTPUT_SEED)\n",
    "torch.manual_seed(OUTPUT_SEED)\n",
    "\n",
    "mu_0 = np.zeros(DIM)\n",
    "mu_T = np.zeros(DIM)\n",
    "mu_optimal_plan = np.zeros(2*DIM)\n",
    "\n",
    "rotation_Y = ortho_group.rvs(DIM)\n",
    "weight_Y = rotation_Y @ np.diag(np.exp(np.linspace(np.log(0.5), np.log(2), DIM)))\n",
    "sigma_Y = weight_Y @ weight_Y.T\n",
    "Y_sampler = distributions.LinearTransformer(distributions.StandartNormalSampler(dim=DIM), weight_Y, bias=None)\n",
    "\n",
    "rotation_X = ortho_group.rvs(DIM)\n",
    "weight_X = rotation_X @ np.diag(np.exp(np.linspace(np.log(0.5), np.log(2), DIM)))\n",
    "sigma_X = weight_X @ weight_X.T\n",
    "X_sampler = distributions.LinearTransformer(distributions.StandartNormalSampler(dim=DIM), weight_X, bias=None)\n",
    "\n",
    "BW = calculate_frechet_distance(np.zeros(DIM), sigma_X, np.zeros(DIM), sigma_Y) / 2\n",
    "print('True Wasserstein-2 Distance: ', BW)\n",
    "\n",
    "X = X_sampler.sample(100000).cpu().detach().numpy()\n",
    "Var_X = np.sum(np.var(X, axis=0))\n",
    "print('Variance of X:', Var_X)\n",
    "      \n",
    "Y = Y_sampler.sample(100000).cpu().detach().numpy()\n",
    "Var_Y = np.sum(np.var(Y, axis=0))\n",
    "print('Variance of Y:', np.sum(Var_Y))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for calculating BW-UVP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrize(X):\n",
    "    return np.real((X + X.T) / 2)\n",
    "\n",
    "def get_D_sigma(covariance_0, covariance_T, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "    \n",
    "    covariance_0_sqrt = symmetrize(sqrtm(covariance_0))\n",
    "    return symmetrize(sqrtm(4*covariance_0_sqrt@covariance_T@covariance_0_sqrt + (epsilon**2)*np.eye(shape)))\n",
    "\n",
    "def get_C_sigma(covariance_0, D_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "    \n",
    "    covariance_0_sqrt = symmetrize(sqrtm(covariance_0))\n",
    "    covariance_0_sqrt_inv = inv(covariance_0_sqrt)\n",
    "    \n",
    "    return 0.5*(covariance_0_sqrt@D_sigma@covariance_0_sqrt_inv - epsilon*np.eye(shape))\n",
    "\n",
    "def get_mu_t(t, mu_0, mu_T):\n",
    "    return (1 - t)*mu_0 + t*mu_T\n",
    "\n",
    "def get_covariance_t(t, covariance_0, covariance_T, C_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "    \n",
    "    return (\n",
    "        ((1-t)**2)*covariance_0 + (t**2)*covariance_T + \n",
    "        t*(1-t)*(C_sigma+C_sigma.T) + epsilon*t*(1-t)*np.eye(shape)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_conditional_covariance_t(t, covariance_0, covariance_T, C_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "    \n",
    "    covariance_0_inv = inv(covariance_0)\n",
    "    \n",
    "    return (\n",
    "        (t**2)*(covariance_T - C_sigma.T@covariance_0_inv@C_sigma) + \n",
    "        epsilon*t*(1-t)*np.eye(shape)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_conditional_mu_t(x0, mu_0, mu_T, t, covariance_0, C_sigma, epsilon):\n",
    "    shape = covariance_0.shape[0]\n",
    "    \n",
    "    covariance_0_inv = inv(covariance_0)\n",
    "    \n",
    "    return (\n",
    "        (1-t)*x0 + t*(mu_T + C_sigma.T@covariance_0_inv@(x0[:, None] - mu_0[:, None]))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_optimal_plan_covariance(covariance_0, covariance_T, C_sigma):\n",
    "    size = covariance_0.shape[0]\n",
    "    optimal_plan_covariance = np.zeros((2*size, 2*size))\n",
    "    \n",
    "    optimal_plan_covariance[:size, :size] = covariance_0\n",
    "    optimal_plan_covariance[size:, size:] = covariance_T\n",
    "    \n",
    "    optimal_plan_covariance[:size, size:] = C_sigma\n",
    "    optimal_plan_covariance[size:, :size] = C_sigma.T\n",
    "    \n",
    "    return optimal_plan_covariance\n",
    "\n",
    "def compute_BW_UVP(samples, true_mu, true_covariance):\n",
    "    samples_covariance = np.cov(samples.T)\n",
    "    samples_mu = samples.mean(axis=0)\n",
    "    samples_covariance_sqrt = symmetrize(sqrtm(samples_covariance))\n",
    "\n",
    "    mu_term = 0.5*((true_mu - samples_mu)**2).sum()\n",
    "    covariance_term = (\n",
    "        0.5*np.trace(samples_covariance) + \n",
    "        0.5*np.trace(true_covariance) -\n",
    "        np.trace(symmetrize(sqrtm(samples_covariance_sqrt@true_covariance@samples_covariance_sqrt)))\n",
    "    )\n",
    "\n",
    "    BW = mu_term + covariance_term\n",
    "    BW_UVP = 100*(BW/(0.5*np.trace(true_covariance)))\n",
    "        \n",
    "    return BW_UVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of parameters for BW-UVP metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_0 = sigma_X\n",
    "covariance_T = sigma_Y\n",
    "\n",
    "D_sigma = get_D_sigma(covariance_0, covariance_T, EPSILON)\n",
    "C_sigma = get_C_sigma(covariance_0, D_sigma, EPSILON)\n",
    "\n",
    "mu_t = np.stack(\n",
    "    [get_mu_t(t, mu_0, mu_T) for t in np.linspace(0, 1, N_STEPS+1)], axis=0\n",
    ")\n",
    "\n",
    "covariance_t = np.stack(\n",
    "    [get_covariance_t(t, covariance_0, covariance_T, C_sigma, EPSILON) for t in np.linspace(0, 1, N_STEPS+1)],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "optimal_plan_covariance = get_optimal_plan_covariance(covariance_0, covariance_T, C_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functions for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.silu(input)\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.scale = scale\n",
    "\n",
    "        inv_freq = torch.exp(\n",
    "            torch.arange(0, dim, 2, dtype=torch.float32) * (-math.log(10000) / dim)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        shape = input.shape\n",
    "        \n",
    "        input = input*self.scale + 1\n",
    "        sinusoid_in = torch.ger(input.view(-1).float(), self.inv_freq)\n",
    "        pos_emb = torch.cat([sinusoid_in.sin(), sinusoid_in.cos()], dim=-1)\n",
    "        pos_emb = pos_emb.view(*shape, self.dim)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "def make_net(n_inputs, n_outputs, n_layers=3, n_hiddens=100):\n",
    "    layers = [nn.Linear(n_inputs, n_hiddens), nn.ReLU()]\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        layers.extend([nn.Linear(n_hiddens, n_hiddens), nn.ReLU()])\n",
    "        \n",
    "    layers.append(nn.Linear(n_hiddens, n_outputs))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class SDE(nn.Module):\n",
    "    def __init__(self, shift_model, epsilon, n_steps, time_dim, is_resnet_generator):\n",
    "        super().__init__()\n",
    "        self.shift_model = shift_model\n",
    "        self.epsilon = epsilon\n",
    "        self.n_steps = n_steps\n",
    "        self.delta_t = 1/n_steps\n",
    "        self.is_resnet_generator = is_resnet_generator\n",
    "        \n",
    "        self.time = nn.Sequential(\n",
    "            TimeEmbedding(time_dim, scale=n_steps),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        if self.is_resnet_generator:\n",
    "            trajectory, shifts = self.shift_model(x0)\n",
    "            times = torch.linspace(0, 1, self.n_steps + 1)\n",
    "            \n",
    "            return trajectory, times, shifts\n",
    "        else:\n",
    "            t0 = 0.0\n",
    "            trajectory = [x0]\n",
    "            times = [t0]\n",
    "            shifts = []\n",
    "\n",
    "            x, t = x0, t0\n",
    "\n",
    "            for step in range(self.n_steps):\n",
    "                x, t, shift = self._step(x, t)\n",
    "\n",
    "                trajectory.append(x)\n",
    "                times.append(t)\n",
    "                shifts.append(shift)\n",
    "\n",
    "            return torch.stack(trajectory, dim=1), torch.tensor(times), torch.stack(shifts, dim=1)\n",
    "    \n",
    "    def _step(self, x, t):\n",
    "        if PREDICT_SHIFT:\n",
    "            shift = self._get_shift(x, t)\n",
    "            shifted_x = x + shift*torch.tensor(self.delta_t).cuda()\n",
    "        else:\n",
    "            shifted_x = self._get_shift(x, t)\n",
    "            shift = (shifted_x - x)/(torch.tensor(self.delta_t).cuda())\n",
    "        noise = self._sample_noise(x)\n",
    "        \n",
    "        return shifted_x + noise, t + self.delta_t, shift\n",
    "    \n",
    "    def _get_shift(self, x, t):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        if CONSTANT_TIME:\n",
    "            t = 0.0\n",
    "        \n",
    "        if USE_POSITIONAL_ENCODING:\n",
    "            t = torch.tensor(t).repeat(batch_size)\n",
    "            t = t.cuda()\n",
    "            t = self.time(t)\n",
    "        else:\n",
    "            t = torch.tensor(t).repeat(batch_size)[:, None]\n",
    "            if x.device.type == \"cuda\":\n",
    "                t = t.cuda()\n",
    "            \n",
    "        x_t = torch.cat((x, t), dim=-1)\n",
    "        \n",
    "        return self.shift_model(x_t)\n",
    "        \n",
    "    def _sample_noise(self, x):\n",
    "        noise = math.sqrt(self.epsilon)*math.sqrt(self.delta_t)*torch.randn(x.shape)\n",
    "        \n",
    "        if x.device.type == \"cuda\":\n",
    "            noise = noise.cuda()\n",
    "        return noise\n",
    "    \n",
    "    def set_n_steps(self, n_steps):\n",
    "        self.n_steps = n_steps\n",
    "        self.delta_t = 1/n_steps\n",
    "        \n",
    "\n",
    "def integrate(values, times):\n",
    "    deltas = times[1:] - times[:-1]\n",
    "    if values.device.type == \"cuda\":\n",
    "        deltas = deltas.cuda()\n",
    "    return (values*deltas[None, :]).sum(dim = 1)\n",
    "\n",
    "\n",
    "def compute_metrics(X_sampler, T, N_STEPS, mu_t, covariance_t):\n",
    "    X = X_sampler.sample(100000)\n",
    "    \n",
    "    trajectory = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(100000//BATCH_SIZE + 1):\n",
    "            trajectory.append(T(X[BATCH_SIZE*i:BATCH_SIZE*(i+1)])[0].cpu().numpy())\n",
    "            \n",
    "    trajectory = np.concatenate(trajectory, axis = 0)\n",
    "    \n",
    "    result = []\n",
    "    for step in range(1, N_STEPS + 1):\n",
    "        T_X = trajectory[:, step, :]\n",
    "        \n",
    "        T_X_covariance = np.cov(T_X.T)\n",
    "        T_X_mu = T_X.mean(axis=0)\n",
    "        \n",
    "        true_mu = mu_t[step]\n",
    "        true_covariance = covariance_t[step]\n",
    "        \n",
    "        T_X_covariance_sqrt = symmetrize(sqrtm(T_X_covariance))\n",
    "        \n",
    "        mu_term = 0.5*((true_mu - T_X_mu)**2).sum()\n",
    "        covariance_term = (\n",
    "            0.5*np.trace(T_X_covariance) + \n",
    "            0.5*np.trace(true_covariance) -\n",
    "            np.trace(symmetrize(sqrtm(T_X_covariance_sqrt@true_covariance@T_X_covariance_sqrt)))\n",
    "        )\n",
    "        \n",
    "        BW = mu_term + covariance_term\n",
    "        BW_UVP = 100*(BW/(0.5*np.trace(true_covariance)))\n",
    "        \n",
    "        result.append(BW_UVP)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def calculate_noise_norm(dim, n_steps, epsilon):\n",
    "    n = dim\n",
    "    dt = 1/n_steps\n",
    "    sigma = math.sqrt(dt)*math.sqrt(epsilon)\n",
    "\n",
    "    return n_steps*(math.exp(math.log(sigma) + math.log(math.sqrt(2)) + math.lgamma((n+1)/2) - math.lgamma(n/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T params: 788740\n",
      "D params: 657409\n"
     ]
    }
   ],
   "source": [
    "D = make_net(DIM, 1, n_layers=D_N_LAYERS, n_hiddens=D_N_HIDDEN).cuda()\n",
    "    \n",
    "T = make_net(DIM+TIME_DIM, DIM, n_layers=T_N_LAYERS, n_hiddens=T_N_HIDDEN).cuda()\n",
    "T = SDE(T, EPSILON, N_STEPS, TIME_DIM, IS_RESNET_GENERATOR).cuda()\n",
    "    \n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('D params:', np.sum([np.prod(p.shape) for p in D.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_opt = torch.optim.Adam(T.parameters(), lr=T_LR, weight_decay=1e-10)\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=D_LR, weight_decay=1e-10)\n",
    "\n",
    "if CONTINUE > -1:\n",
    "    T_opt.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'T_opt_{SEED}_{CONTINUE}.pt')))\n",
    "    T.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'T_{SEED}_{CONTINUE}.pt')))\n",
    "    D_opt.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'D_opt_{SEED}_{CONTINUE}.pt')))\n",
    "    D.load_state_dict(torch.load(os.path.join(OUTPUT_PATH, f'D_{SEED}_{CONTINUE}.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name=EXP_NAME, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "noise_norm = calculate_noise_norm(DIM, N_STEPS, EPSILON)\n",
    "wandb.log({f'Noise norm' : noise_norm}, step=0)\n",
    "\n",
    "for step in tqdm(range(CONTINUE + 1, MAX_STEPS)):\n",
    "    unfreeze(T); freeze(D)\n",
    "    for t_iter in range(T_ITERS):\n",
    "        T_opt.zero_grad()\n",
    "        \n",
    "        X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "        X0.requires_grad_()\n",
    "        \n",
    "        trajectory, times, shifts = T(X0)\n",
    "        XN = trajectory[:, -1]\n",
    "        norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1)**2\n",
    "        integral = INTEGRAL_SCALE*integrate(norm, times)\n",
    "        \n",
    "        T_loss = (integral + D(X1) - D(XN)).mean()\n",
    "        T_loss.backward()\n",
    "        T_gradient_norm = torch.nn.utils.clip_grad_norm_(T.parameters(), max_norm=T_GRADIENT_MAX_NORM)\n",
    "        T_opt.step()\n",
    "    \n",
    "    wandb.log({f'T gradient norm' : T_gradient_norm.item()}, step=step)\n",
    "    wandb.log({f'Mean norm' : torch.sqrt(norm).mean().item()}, step=step)\n",
    "    wandb.log({f'T_loss' : T_loss.item()}, step=step)\n",
    "        \n",
    "    del T_loss, X0, X1, XN; gc.collect(); torch.cuda.empty_cache()\n",
    "        \n",
    "    freeze(T); unfreeze(D)\n",
    "    \n",
    "    D_opt.zero_grad()\n",
    "    \n",
    "    X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "    trajectory, times, shifts = T(X0)\n",
    "    XN = trajectory[:, -1]\n",
    "    norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1)**2\n",
    "    integral = INTEGRAL_SCALE*integrate(norm, times)\n",
    "    \n",
    "    D_X1 = D(X1)\n",
    "    D_XN = D(XN)\n",
    "    \n",
    "    D_loss = (-integral - D_X1 + D_XN).mean()\n",
    "    D_loss.backward()\n",
    "    D_gradient_norm = torch.nn.utils.clip_grad_norm_(D.parameters(), max_norm=D_GRADIENT_MAX_NORM)\n",
    "    D_opt.step()\n",
    "    \n",
    "    wandb.log({f'D gradient norm' : D_gradient_norm.item()}, step=step)\n",
    "    wandb.log({f'D_loss' : D_loss.item()}, step=step)\n",
    "    \n",
    "    wandb.log({f'integral' : integral.mean().item()}, step=step)\n",
    "    wandb.log({f'D_X1' : D_X1.mean().item()}, step=step)\n",
    "    wandb.log({f'D_XN' : D_XN.mean().item()}, step=step)\n",
    "    del D_loss, X0, X1, XN; gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        metrics.append(compute_metrics(X_sampler, T, N_STEPS, mu_t, covariance_t))\n",
    "                \n",
    "        for i in range(N_STEPS):\n",
    "            wandb.log({f'BW_UVP_{i}' : metrics[-1][i]}, step=step)\n",
    "            \n",
    "        wandb.log({f'BW_UVP_mean' : np.mean(metrics[-1])}, step=step)\n",
    "        wandb.log({f'BW_UVP_max' : max(metrics[-1])}, step=step)\n",
    "        \n",
    "        X = X_sampler.sample(100000)\n",
    "        \n",
    "        trajectory = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(100000//BATCH_SIZE + 1):\n",
    "                trajectory.append(T(X[BATCH_SIZE*i:BATCH_SIZE*(i+1)])[0].cpu().numpy())\n",
    "\n",
    "        trajectory = np.concatenate(trajectory, axis = 0)\n",
    "        \n",
    "        TX = trajectory[:, -1]\n",
    "        X = X.cpu().numpy()\n",
    "        X_TX = np.concatenate((X, TX), axis=1)\n",
    "        \n",
    "        BW_UVP_target_distr = compute_BW_UVP(TX, mu_T, covariance_T)\n",
    "        wandb.log({f'BW_UVP_target_distr' : BW_UVP_target_distr}, step=step)\n",
    "        \n",
    "        BW_UVP_optimal_plan = compute_BW_UVP(X_TX, mu_optimal_plan, optimal_plan_covariance)\n",
    "        wandb.log({f'BW_UVP_optimal_plan' : BW_UVP_optimal_plan}, step=step)\n",
    "        \n",
    "\n",
    "    gc.collect(); torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
